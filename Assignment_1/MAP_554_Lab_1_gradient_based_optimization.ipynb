{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNZAexoZAZOy"
   },
   "source": [
    "#  MAP 554 / TP1 -  Lab 1: Gradient-based optimization\n",
    "\n",
    "\n",
    "This notebook is written as an introduction to gradient-based optimization with Python. \n",
    "\n",
    "This notebook consists of three parts: \n",
    "- Design of a steepest descent algorithm\n",
    "- Implementation of a linear search for step size search\n",
    "- Use of library `Scipy` for optimization\n",
    "- Several exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uldz8ZqPAZO2"
   },
   "source": [
    "## 0- Definition of test functions\n",
    "`Parabola`: 1D parabolic function  $z \\rightarrow z^2$ \n",
    "\n",
    "`Rosenbrock`: 2D function $\\mathbf{z} \\rightarrow 100\\times(z_2-z_1^2)^2+(1-z_1)^2$\n",
    "\n",
    "`Ackley`: 2D function $\\mathbf{z} \\rightarrow -20 \\exp\\left({-0.2 \\sqrt{0.5(z_1^2+z_2^2)}}\\right) - \\exp\\left(0.5\\cos(2\\pi z_1) + 0.5\\cos(2\\pi z_2)\\right) + e + 20$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOMbBDX6AZO6"
   },
   "outputs": [],
   "source": [
    "# Support for maths\n",
    "import numpy as np\n",
    "\n",
    "def Parabole(z):\n",
    "    return z**2\n",
    "\n",
    "def Rosenbrock(z):\n",
    "    z1 = z[0]\n",
    "    z2 = z[1]\n",
    "    return 100*(z[1]-z[0]**2)**2+(1-z[0])**2\n",
    "\n",
    "def compos(z):\n",
    "    return Parabole(Rosenbrock(z))\n",
    "\n",
    "def Ackley(z):\n",
    "    return -20 * np.exp(-0.2*np.sqrt(0.5*(z[0]**2+z[1]**2))) - np.exp(0.5*np.cos(2*np.pi*z[0])+0.5*np.cos(2*np.pi*z[1])) + np.exp(1) + 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maaV-3faAZPO"
   },
   "source": [
    "The gradient of the different functions will be computed thanks to the package `Autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVAkVOnkAZPS"
   },
   "outputs": [],
   "source": [
    "#Automatic gradient computation\n",
    "import autograd\n",
    "import autograd.numpy as np\n",
    "\n",
    "#definition of functions for gradient\n",
    "nabla_Parabole=autograd.grad(Parabole)\n",
    "\n",
    "nabla_Rosenbrock=autograd.grad(Rosenbrock)\n",
    "\n",
    "nabla_Ackley=autograd.grad(Ackley)\n",
    "\n",
    "nabla_compos=autograd.grad(compos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LgqZBZ_AZPc"
   },
   "source": [
    "Optima for the different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQcyVNf9AZPc"
   },
   "outputs": [],
   "source": [
    "Optimum_Parabole = 0.\n",
    "Optimum_Rosenbrock = np.array([1., 1.])\n",
    "Optimum_Ackley = np.array([0., 0.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY1RddqAZPk"
   },
   "source": [
    "Visualization of 2D functions with the package `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "BNS_AAvEAZPm",
    "outputId": "773f9bb1-c41a-4ce4-a00c-487d8c38373f"
   },
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "##### Plot Parabola #######\n",
    "X_parabole=np.linspace(-1,1,100)\n",
    "Y_parabole=np.array([Parabole(x) for x in X_parabole])\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.plot(X_parabole,Y_parabole)\n",
    "plt.plot(Optimum_Parabole,Parabole(Optimum_Parabole), 'r*', markersize=10)\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Parabole')\n",
    "\n",
    "##### Plot Contour plot Rosenbrock & Ackley#######\n",
    "from matplotlib.colors import LogNorm,PowerNorm\n",
    "import matplotlib.colors as colors\n",
    "xmin, xmax, xstep = -5.,5., .2\n",
    "ymin, ymax, ystep = -5., 5., .2\n",
    "X, Y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n",
    "##### Plot Rosenbrock 2D ######\n",
    "plt.subplot(132)\n",
    "z_rosenbrock = np.array([Rosenbrock(np.array([x,y])) for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
    "Z_rosenbrock  = z_rosenbrock.reshape(X.shape)\n",
    "CS=plt.contour(X, Y, Z_rosenbrock, levels=np.logspace(0, 8, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "plt.plot([Optimum_Rosenbrock[0]],[Optimum_Rosenbrock[1]], 'r*', markersize=18)\n",
    "plt.clabel(CS,inline=1,fontsize=10)\n",
    "plt.pcolormesh(X,Y,Z_rosenbrock,shading='gouraud',norm=PowerNorm(gamma=0.3),cmap=plt.cm.jet)\n",
    "plt.xlabel('$z_1$')\n",
    "plt.ylabel('$z_2$')\n",
    "plt.title('Rosenbrock')\n",
    "#plt.colorbar()\n",
    "##### Plot Ackley 2D ######\n",
    "plt.subplot(133)\n",
    "z_ackley = np.array([Ackley(np.array([x,y])) for x, y in zip(np.ravel(X), np.ravel(Y))])\n",
    "Z_ackley = z_ackley.reshape(X.shape)\n",
    "CS=plt.contour(X, Y, Z_ackley, levels=np.logspace(0, 8, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
    "plt.plot([Optimum_Ackley[0]],[Optimum_Ackley[1]], 'r*', markersize=18)\n",
    "plt.clabel(CS,inline=1,fontsize=10)\n",
    "plt.pcolormesh(X,Y,Z_ackley,shading='gouraud',cmap=plt.cm.jet)\n",
    "plt.xlabel('$z_1$')\n",
    "plt.ylabel('$z_2$')\n",
    "plt.title('Ackley')\n",
    "\n",
    "#plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SFWyO6VAZPu"
   },
   "source": [
    "3D view of Rosenbrock and Ackley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "U1iZst31AZPw",
    "outputId": "e6095b9f-ebd2-4c75-c798-904f363a1375"
   },
   "outputs": [],
   "source": [
    "# import 3D vizualization tools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "\n",
    "###### Rosenbrock 3D Vizualization######\n",
    "ax1 = fig.add_subplot(121,projection='3d', elev=50, azim=-50)\n",
    "ax1.plot_surface(X, Y, Z_rosenbrock , norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "ax1.plot([Optimum_Rosenbrock[0]],[Optimum_Rosenbrock[1]], [Rosenbrock(Optimum_Rosenbrock)], 'r*', markersize=10)\n",
    "ax1.set_xlabel('$z_1$')\n",
    "ax1.set_ylabel('$z_2$')\n",
    "ax1.set_zlabel('$y$')\n",
    "\n",
    "ax1.set_xlim((xmin, xmax))\n",
    "ax1.set_ylim((ymin, ymax))\n",
    "ax1.set_title('Rosenbrock 2D')\n",
    "\n",
    "###### Ackley 3D Vizualization######\n",
    "ax2 = fig.add_subplot(122,projection='3d', elev=50, azim=-50)\n",
    "\n",
    "ax2.plot_surface(X, Y, Z_ackley , norm=LogNorm(), rstride=1, cstride=1, \n",
    "                edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n",
    "ax2.plot([Optimum_Ackley[0]],[Optimum_Ackley[1]], [Ackley(Optimum_Ackley)], 'r*', markersize=10)\n",
    "ax2.set_xlabel('$z_1$')\n",
    "ax2.set_ylabel('$z_2$')\n",
    "ax2.set_zlabel('$y$')\n",
    "\n",
    "ax2.set_xlim((xmin, xmax))\n",
    "ax2.set_ylim((ymin, ymax))\n",
    "ax2.set_title('Ackley 2D')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKuGqnxNAZP2"
   },
   "source": [
    "## 1- Steepest Gradient descent\n",
    "Basic algorithm of gradient based descent. The considered descent direction is $\\mathbf{S}^{(k)} = -\\nabla f(z^{(k)}) $ with $k$ le number of current iteration. The stepsize is noted $\\alpha^{(k)}$. The update of the optimziation variables is performed as fallows:\n",
    "$$\\mathbf{z}^{(k+1)} = \\mathbf{z}^{(k)} - \\alpha^{(k)}\\nabla f(z^{(k)})$$\n",
    "\n",
    "`gd_steepest`: function to define which provides the $\\mathbf{z}^{(k)}$. This function takes as arguments the initial point `z0`, the gradient computation function`grad`, the value of the stepsize `alpha` and the maximal number of iterations `max_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sP2LKHZAZP4"
   },
   "outputs": [],
   "source": [
    "# Definition of steepest descent algorithm with constant step size\n",
    "# The algorithm ends with the maximal number of iterations is reached\n",
    "def gd_steepest(z0, grad, alpha, max_iter=100):\n",
    "    \n",
    "    #dimension de z\n",
    "    d=z0.shape[0]\n",
    "    zs = np.zeros((1 + max_iter,d))\n",
    "    zs[0] = z0\n",
    "    z = z0\n",
    "    #boucle sur le nombre d'iterations\n",
    "    for i in range(max_iter):\n",
    "        #calcul du nouveau point\n",
    "        z = z - alpha * grad(z)\n",
    "        zs[i+1] = z\n",
    "    return zs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8MldhHjAZP-"
   },
   "outputs": [],
   "source": [
    "#Definition of initial points\n",
    "z0_parabole = np.array([0.9])\n",
    "z0_rosenbrock = np.array([0.5,-0.8])\n",
    "z0_ackley = np.array([0.5,-0.8])\n",
    "\n",
    "#Definition of step size\n",
    "alpha_parabole = 1e-3\n",
    "alpha_rosenbrock = 1e-4\n",
    "alpha_ackley = 1.e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYFDrzemAZQC"
   },
   "outputs": [],
   "source": [
    "# Run of optimization algorithm on different functions\n",
    "gd_path_parabole = gd_steepest(z0_parabole,nabla_Parabole,alpha_parabole,max_iter=50)\n",
    "gd_path_rosenbrock = gd_steepest(z0_rosenbrock,nabla_Rosenbrock,alpha_rosenbrock,max_iter=50)\n",
    "gd_path_ackley = gd_steepest(z0_ackley,nabla_Ackley,alpha_ackley,max_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUy95gofAZQQ"
   },
   "source": [
    "Visualization of the gradient descent path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "FkepvtKUAZQS",
    "outputId": "339799dd-4cae-42b8-cf8f-d63f18bd2b3b"
   },
   "outputs": [],
   "source": [
    "#### Path of the algorithme on the parabola function ####\n",
    "f_parabole=np.array([Parabole(z) for z in gd_path_parabole])\n",
    "path_parabole=np.concatenate((gd_path_parabole,f_parabole))\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_parabole,Y_parabole)\n",
    "plt.plot(Optimum_Parabole,Parabole(Optimum_Parabole), 'r*', markersize=18)\n",
    "plt.plot(z0_parabole,Parabole(z0_parabole), 'bo', markersize=18)\n",
    "plt.quiver(path_parabole[0,:-1], path_parabole[1,:-1], path_parabole[0,1:]-path_parabole[0,:-1], path_parabole[1,1:]-path_parabole[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "9Nu4M3UQAZQY",
    "outputId": "e465278e-8bf5-4d1c-dca6-340ded483938"
   },
   "outputs": [],
   "source": [
    "#### Path of the algorithm on the Rosenbrock function ####\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "CS=ax.contour(X, Y, Z_rosenbrock, levels=np.logspace(0, 8, 35), norm=LogNorm(), cmap=plt.cm.jet,alpha=0.3)\n",
    "ax.plot([Optimum_Rosenbrock[0]],[Optimum_Rosenbrock[1]], 'r*', markersize=18)\n",
    "ax.plot([z0_rosenbrock[0]],[z0_rosenbrock[1]], 'bo', markersize=18)\n",
    "plt.clabel(CS,inline=1,fontsize=10)\n",
    "plt.pcolormesh(X,Y,Z_rosenbrock,shading='gouraud',norm=PowerNorm(gamma=0.3),cmap=plt.cm.jet,alpha=0.3)\n",
    "ax.quiver(gd_path_rosenbrock[0,:-1], gd_path_rosenbrock[1,:-1], gd_path_rosenbrock[0,1:]-gd_path_rosenbrock[0,:-1], gd_path_rosenbrock[1,1:]-gd_path_rosenbrock[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "68IBx35oAZQi",
    "outputId": "c9ebc639-3767-4dc0-82fb-3912caf8f769"
   },
   "outputs": [],
   "source": [
    "#### Path of the algorithm on the Ackley function ####\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "CS=ax.contour(X, Y, Z_ackley, levels=np.logspace(0, 8, 35), norm=LogNorm(), cmap=plt.cm.jet,alpha=0.3)\n",
    "ax.plot([Optimum_Ackley[0]],[Optimum_Ackley[1]], 'r*', markersize=18)\n",
    "ax.plot([z0_ackley[0]],[z0_ackley[1]], 'bo', markersize=18)\n",
    "plt.clabel(CS,inline=1,fontsize=10)\n",
    "plt.pcolormesh(X,Y,Z_ackley,shading='gouraud',cmap=plt.cm.jet,alpha=0.3)\n",
    "ax.quiver(gd_path_ackley[0,:-1], gd_path_ackley[1,:-1], gd_path_ackley[0,1:]-gd_path_ackley[0,:-1], gd_path_ackley[1,1:]-gd_path_ackley[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG-XZc6FAZQs"
   },
   "source": [
    "## Short exercice: Try different values of step size on the functions and conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-z5bbVqAZQu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw4y54FuAZQy"
   },
   "source": [
    "## 2- Linear search of  Wolfe\n",
    "To avoid too large step size, verify the Armijo condition:\n",
    "$$f\\left(\\textbf{z}+\\alpha S\\right)\\leq f(\\textbf{z})+\\epsilon_1 \\alpha\\nabla f(\\textbf{z})^T $$\n",
    "\n",
    "To avoid too small step size, verify the curvature condition:\n",
    "$$S^T \\nabla f(\\textbf{z}+\\alpha S)\\geq \\epsilon_2 S^T \\nabla f(\\textbf{z}) $$\n",
    "\n",
    "\n",
    "'Backtracking' algoritjm:\n",
    "\n",
    "A step size $\\alpha^*$ that verifies the Wolfe's conditions:\n",
    "- 1 $i=0 ; \\alpha_-=0 ; \\alpha_+=+\\infty;\\epsilon_1=10^{-4};\\epsilon_2=0.99 $;\n",
    "- 2 While $\\alpha_i$  does not verify the Wolfe's conditions:\n",
    "    - If the step size is too large $\\alpha_+=\\alpha_i$ et $\\alpha_{i+1}=\\frac{\\alpha_-+\\alpha_+}{2}$\n",
    "    - If the step size is too small $\\alpha_-=\\alpha_i$ et $\\alpha_{i+1}=\\frac{\\alpha_-+\\alpha_+}{2}$ si $\\alpha_+<+\\infty$; $\\alpha_{i+1}=2\\alpha_i$ else.\n",
    "- 3 i=i+1\n",
    "- 4 return $\\alpha_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CVqe4-OAZQy"
   },
   "outputs": [],
   "source": [
    "#Backtracking linear search\n",
    "# arguments : current point z, function to be optimized, gradient of the function, initial value of the step size\n",
    "\n",
    "def linear_search(z,func,grad,alpha0):\n",
    "    alphamax=np.inf\n",
    "    alphamin=0\n",
    "    i=0\n",
    "    alpha=alpha0\n",
    "    epsilon_1 = 1e-4\n",
    "    epsilon_2 = 0.99\n",
    "    while  i<=100 and (func(z-alpha0*grad(z))>func(z) + epsilon_1 * np.dot(np.array([alpha0]*z.shape[0]).reshape(1,z.shape[0]),grad(z).reshape(z.shape[0],1))or\\\n",
    "                       np.dot(-grad(z).reshape(1,z.shape[0]),grad(z-alpha0*grad(z)).reshape(z.shape[0],1))<-epsilon_2*np.dot(grad(z).reshape(1,z.shape[0]),grad(z).reshape(z.shape[0],1))):\n",
    "        if func(z-alpha0*grad(z))>func(z) + epsilon_1 *np.dot(np.array([alpha0]*z.shape[0]).reshape(1,z.shape[0]),grad(z).reshape(z.shape[0],1)):\n",
    "            alphamax = alpha0\n",
    "            alpha = (alphamax+alphamin)/2.\n",
    "            \n",
    "        else:\n",
    "            if np.dot(-grad(z).reshape(1,z.shape[0]),grad(z-alpha0*grad(z)).reshape(z.shape[0],1))<-epsilon_2*np.dot(grad(z).reshape(1,z.shape[0]),grad(z).reshape(z.shape[0],1)):\n",
    "                alphamin=alpha0 \n",
    "                if alphamax<np.inf:\n",
    "                    alpha = (alphamax+alphamin)/2.\n",
    "                else:\n",
    "                    alpha = 2*alpha0\n",
    "        alpha0=alpha\n",
    "        i=i+1\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bmp810e3AZQ4"
   },
   "outputs": [],
   "source": [
    "# Definition of steepest descent algorithm with linear search of step size\n",
    "# arguments : initial point z0, the function to be optimized, the gradient of the function, the initial value of the step size, number maximal of iterations\n",
    "def gd_steepest_variable_stepsize(z0, func, grad, alpha0, max_iter=100):\n",
    "    d=z0.shape[0]\n",
    "    zs = np.zeros((1 + max_iter,d))\n",
    "    zs[0] = z0\n",
    "    z = z0\n",
    "    for i in range(max_iter):\n",
    "        alpha = linear_search(z,func,grad,alpha0)\n",
    "        z = z - alpha * grad(z)\n",
    "        zs[i+1] = z\n",
    "    return zs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsfJo3ZsAZQ8"
   },
   "outputs": [],
   "source": [
    "# Run of optimization algorithm on test functions\n",
    "alpha0_parabole = 2.\n",
    "alpha0_rosenbrock = 2.\n",
    "alpha0_ackley = 2.\n",
    "gd_path_parabole_variable_stepsize = gd_steepest_variable_stepsize(z0_parabole,Parabole,nabla_Parabole,alpha0_parabole,max_iter=50)\n",
    "gd_path_rosenbrock_variable_stepsize = gd_steepest_variable_stepsize(z0_rosenbrock,Rosenbrock,nabla_Rosenbrock,alpha0_rosenbrock,max_iter=50)\n",
    "gd_path_ackley_variable_stepsize = gd_steepest_variable_stepsize(z0_ackley,Ackley,nabla_Ackley,alpha0_ackley,max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "LnMB0GqUAZRA",
    "outputId": "b3db7800-429a-48c4-8d12-3e4d0ea7c0dd"
   },
   "outputs": [],
   "source": [
    "#### Path of the algorithm on the Parabola ####\n",
    "f_parabole=np.array([Parabole(x) for x in gd_path_parabole_variable_stepsize])\n",
    "path_parabole=np.concatenate((gd_path_parabole_variable_stepsize,f_parabole))\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_parabole,Y_parabole)\n",
    "plt.plot(Optimum_Parabole,Parabole(Optimum_Parabole), 'r*', markersize=18)\n",
    "plt.plot(z0_parabole,Parabole(z0_parabole), 'bo', markersize=18)\n",
    "plt.quiver(path_parabole[0,:-1], path_parabole[1,:-1],\\\n",
    "           path_parabole[0,1:]-path_parabole[0,:-1],\\\n",
    "           path_parabole[1,1:]-path_parabole[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$y$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "1rSRv785AZRC",
    "outputId": "c9b2e9ee-9520-44d5-cb10-05589435ea4e"
   },
   "outputs": [],
   "source": [
    "#### Path of the algorithm on the Rosenbrock function ####\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "CS=ax.contour(X, Y, Z_rosenbrock, levels=np.logspace(0, 8, 35), norm=LogNorm(), cmap=plt.cm.jet,alpha=0.5)\n",
    "ax.plot([Optimum_Rosenbrock[0]],[Optimum_Rosenbrock[1]], 'r*', markersize=18)\n",
    "ax.plot([z0_rosenbrock[0]],[z0_rosenbrock[1]], 'bo', markersize=18)\n",
    "plt.clabel(CS,inline=1,fontsize=10)\n",
    "plt.axis('equal')\n",
    "plt.pcolormesh(X,Y,Z_rosenbrock,shading='gouraud',norm=PowerNorm(gamma=0.3),cmap=plt.cm.jet,alpha=0.5)\n",
    "ax.quiver(gd_path_rosenbrock_variable_stepsize[0,:-1], gd_path_rosenbrock_variable_stepsize[1,:-1],\\\n",
    "          gd_path_rosenbrock_variable_stepsize[0,1:]-gd_path_rosenbrock_variable_stepsize[0,:-1],\\\n",
    "          gd_path_rosenbrock_variable_stepsize[1,1:]-gd_path_rosenbrock_variable_stepsize[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "GZ1_PVS-AZRG",
    "outputId": "1cd2bea2-1284-4dfb-ebcb-69acd1cb4c79"
   },
   "outputs": [],
   "source": [
    "#### Path of the algorithm on the Ackley function####\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "CS=ax.contour(X, Y, Z_ackley, levels=np.logspace(0, 8, 35), norm=LogNorm(), cmap=plt.cm.jet,alpha=0.5)\n",
    "ax.plot([Optimum_Ackley[0]],[Optimum_Ackley[1]], 'r*', markersize=18)\n",
    "ax.plot([z0_ackley[0]],[z0_ackley[1]], 'bo', markersize=18)\n",
    "plt.clabel(CS,inline=1,fontsize=10)\n",
    "plt.axis('equal')\n",
    "plt.pcolormesh(X,Y,Z_ackley,shading='gouraud',cmap=plt.cm.jet,alpha=0.5)\n",
    "ax.quiver(gd_path_ackley_variable_stepsize[0,:-1], gd_path_ackley_variable_stepsize[1,:-1],\\\n",
    "          gd_path_ackley_variable_stepsize[0,1:]-gd_path_ackley_variable_stepsize[0,:-1],\\\n",
    "          gd_path_ackley_variable_stepsize[1,1:]-gd_path_ackley_variable_stepsize[1,:-1],\\\n",
    "          scale_units='xy', angles='xy', scale=1, color='k')\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lasuJnnAZRK"
   },
   "source": [
    "## 3- Example of use of optimization algorithms with the package 'Scipy'\n",
    "\n",
    "The package `Scipy.optimize` provides several gradient-based optimization algorithms.  \n",
    "\n",
    "The classical syntax to perform minimization with `Scipy.optimize` is:\n",
    "\n",
    "`scipy.optimize.minimize(fun, x0, args=(), method='Algorithme', jac=None, hess=None, bounds=None, constraints=(), tol=None, callback=None, options=None)`\n",
    "- `fun` : the function to be optimized.\n",
    "- `x0`: initial point.\n",
    "- `args`: Extra arguments for the objective function.\n",
    "- `Method`: the used optimization algorithm:\n",
    "        - ‘CG’ : Conjugate Gradient\n",
    "        - ‘BFGS’ : Method of Broyden-Fletcher-Goldfarb-Shanno (approximation of the Hessian)\n",
    "        - ‘Newton-CG’ : Newton method based on CG\n",
    "        - ‘L-BFGS-B’: Limited memory bounded BFGS.\n",
    "- 'tol' : tolerance of the algorithm (stopping criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5GaQdoRAZRK",
    "outputId": "1b88773a-aae8-4c07-a16a-db8ebfd8a6a5"
   },
   "outputs": [],
   "source": [
    "#import of the package\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "#Run of 'BFGS' on the Rosenbrock function\n",
    "res_Rosenbrock = minimize(Rosenbrock, x0=np.array([-4,4]), method='BFGS',\n",
    "                          jac=nabla_Rosenbrock, tol=1e-20)\n",
    "res_Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-HefQdzAZRM"
   },
   "outputs": [],
   "source": [
    "#### Function used to save the path of the algorithm\n",
    "def make_minimize_cb(path=[]):\n",
    "    \n",
    "    def minimize_cb(xk):\n",
    "        path.append(np.copy(xk))\n",
    "\n",
    "    return minimize_cb\n",
    "\n",
    "path_ = [np.array([6,6])]\n",
    "\n",
    "res_Rosenbrock = minimize(Rosenbrock, x0=np.array([-2,2]), method='BFGS',\n",
    "               jac=nabla_Rosenbrock, tol=1e-20,callback=make_minimize_cb(path_))\n",
    "gd_path_rosenbrock = np.array(path_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "_p_Q1ZO_AZRQ",
    "outputId": "3e9c752e-a9ea-4d9e-a64a-1d01b55e9e23"
   },
   "outputs": [],
   "source": [
    "# Path of the optimization algorithm on the Rosenbrock function\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "CS=ax.contour(X, Y, Z_rosenbrock, levels=np.logspace(0, 8, 35), norm=LogNorm(), cmap=plt.cm.jet,alpha=0.5)\n",
    "#ax.plot([Optimum_Rosenbrock[0]],[Optimum_Rosenbrock[1]], 'r*', markersize=18)\n",
    "#ax.plot([z0_rosenbrock[0]],[z0_rosenbrock[1]], 'bo', markersize=18)\n",
    "plt.clabel(CS,inline=1,fontsize=10)\n",
    "plt.pcolormesh(X,Y,Z_rosenbrock,shading='gouraud',norm=PowerNorm(gamma=0.3),cmap=plt.cm.jet,alpha=0.5)\n",
    "ax.quiver(gd_path_rosenbrock[0,:-1], gd_path_rosenbrock[1,:-1], gd_path_rosenbrock[0,1:]-gd_path_rosenbrock[0,:-1], gd_path_rosenbrock[1,1:]-gd_path_rosenbrock[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGVs5l5HAZRW"
   },
   "source": [
    "# Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDDdESTlAZRW"
   },
   "source": [
    "- Code the algorithms 'BFGS' and 'Newton-CG' on the Ackley function from different initializations\n",
    "    \n",
    "- Conclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tVvRHSrCAZRW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MAP 554 - Lab 1 gradient based optimization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
